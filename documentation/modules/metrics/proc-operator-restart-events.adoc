// suppress inspection "KubernetesDeprecatedKeys" for whole file
// suppress inspection "KubernetesNonEditableKeys" for whole file
// suppress inspection "KubernetesUnknownKeys" for whole file
// Module included in the following assemblies:
//
// metrics/assembly-metrics.adoc

[id='proc-operator-restart-events-{context}']
= Monitoring Strimzi intitiated Kafka restarts

[role="_abstract"]
When the cluster operator restarts a Kafka pod, it emits a Kubernetes event into the pod's namespace explaining why it did so.


== Observing restart events

Using `kubectl` you can list events, and filter for only those emitted by the cluster operator by setting the `source` field.

[source,shell-session]
----
~/ > kubectl -n kafka get events --field-selector source=strimzi.io/cluster-operator
LAST SEEN   TYPE     REASON                   OBJECT                        MESSAGE
2m          Normal   JbodVolumesChanged       pod/strimzi-cluster-kafka-0   JBOD volumes were added or removed
58m         Normal   PodForceRestartOnError   pod/strimzi-cluster-kafka-1   Pod needs to be forcibly restarted due to an error
5m47s       Normal   ManualRollingUpdate      pod/strimzi-cluster-kafka-2   Pod was manually annotated to be rolled
----

To view more detailed information about an event or events, use an output format such as yaml. Note that this example is also filtering
on `reason` to further limit the returned events.

[source,shell-session]
~/ > kubectl -n kafka get events --field-selector source=strimzi.io/cluster-operator,reason=PodForceRestartOnError -o yaml

[source,yaml]
----
apiVersion: v1
items:
- action: StrimziInitiatedPodRestart
  apiVersion: v1
  eventTime: "2022-05-13T00:22:34.168086Z"
  firstTimestamp: null <1>
  involvedObject:
      kind: Pod
      name: strimzi-cluster-kafka-1
      namespace: kafka
  kind: Event
  lastTimestamp: null <1>
  message: Pod needs to be forcibly restarted due to an error
  metadata:
      creationTimestamp: "2022-05-13T00:22:34Z"
      generateName: strimzi-event
      name: strimzi-eventwppk6
      namespace: kafka
      resourceVersion: "432961"
      uid: 29fcdb9e-f2cf-4c95-a165-a5efcd48edfc
  reason: PodForceRestartOnError
  reportingComponent: strimzi.io/cluster-operator
  reportingInstance: strimzi-cluster-operator-6458cfb4c6-6bpdp
  source: {} <1>
  type: Normal
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
----
<1> These fields are deprecated, and will be removed in Kubernetes 1.25. As such, they are not populated for these events.

=== Using field selectors with Kubernetes events

The event APIs of Kubernetes are changing, so the event fields that can be selected to filter on are also changing names.


==== Kubernetes 1.24 and earlier

The following fields are available when filtering events with  `--field-selector`:

* `involvedObject.kind`: the involved object is the pod that was restarted, and for these events, the kind will always be "Pod".
* `involvedObject.namespace`: the namespace that the pod belongs to.
* `involvedObject.name`: the pod's name, for example, `strimzi-cluster-kafka-0`.
* `involvedObject.uid`: the unique id of the pod.
* `reason`: why the pod was restarted, for example, `JbodVolumesChanged`.
* `reportingComponent`: For Strimzi restart events, this will always be `strimzi.io/cluster-operator`.
* `source`: An older version of the above field, this will also always be `strimzi.io/cluster-operator`.
* `type`: This can be either `Warning` or `Normal`, for all current Strimzi restart events, it is currently always `Normal`.

==== Kubernetes 1.25+

When Kubernetes 1.25 is released and installed on your cluster, you will also be able to use these fields with  `--field-selector`:

* `regarding.kind`: same as `involvedObject.kind`
* `regarding.namespace`: same as `involvedObject.namespace`
* `regarding.name`: same as `involvedObject.name`
* `regarding.name`: same as `involvedObject.uid`
* `reportingController`: same as `reportingComponent`

== Current reasons why Strimzi will restart a Kafka pod

* `CaCertHasOldGeneration` -  The pod is still using a server certificate signed with an old CA, so needs to be restarted as part of updating that certificate
* `CaCertRemoved` -  Expired CA certificates have been removed, and the pod is restarted to run with the current certificates
* `CaCertRenewed` -  CA certificates have been renewed, and the pod is restarted to run with these updated certificates
* `ClientCaCertKeyReplaced` -  The key used to sign client CA certificates has been replaced, and the pod is being restarted as part of the CA renewal process
* `ClusterCaCertKeyReplaced` -  The key used to sign the cluster's CA certificates has been replaced, and the pod is being restarted as part of the CA renewal process
* `ConfigChangeRequiresRestart` -  Some Kafka configuration properties can be changed dynamically, but others require that the broker be restarted
* `CustomListenerCaCertChanged` -  CA certificates used to secure the Kafka network listeners has changed, and the pod is restarted to use it
* `FileSystemResizeNeeded` -  The file system size has been increased, and a restart is needed to apply it
* `JbodVolumesChanged` -  A disk was added or removed from the Kafka volumes, and a restart is needed to apply the change
* `ManualRollingUpdate` -  A user annotated the pod, or the stateful set / strimzi pod set it belongs to, to trigger a restart
* `PodForceRestartOnError` -  An error occurred that requires restarting the pod to rectify
* `PodHasOldGeneration` -  The stateful set that the pod is a member of has been updated, so the pod needs to be recreated
* `PodHasOldRevision` -  The strimzi pod set that the pod is a member of has been updated, so the pod needs to be recreated
* `PodStuck ` -  The pod is still pending, and either unschedulable, or not scheduled, so the operator is going to restart it as a last ditch effort.
* `PodUnresponsive` -  Strimzi was unable to connect to the pod, which can indicate a broker not coming up properly, so the operator will restart it to try to resolve the issue
* `KafkaCertificatesChanged` -  One or more TLS certificates used by the Kafka broker have been updated, and a restart is needed to use them

== Monitoring restart events

There are multiple open source projects that export Kubernetes events as metrics consumable by popular tools such as Prometheus, searching the web for "Kubernetes events exporter" will yield multiple projects you can use.
